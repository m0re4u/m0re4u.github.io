+++
date = "2019-02-28T15:42:30+01:00"
draft = false
title = "The Growth of High-Performance Machine Learning"
+++

Machine learning, the hype that is being talked about around most of the water coolers, feels like a magic box for most laymen. Artificial Intelligence (AI) is on the rise more than ever, and for most people it seemed to come out of nowhere. However, what they don't know is that most of the theoretical work was already laid out over 50 years ago. The technology that is in use today didn't come out of nowhere, but is the result of years of research in computer science, psychology and mathematics. Nowadays, these model push existing soft- and hardware to the limit for the purpose of

## AI Winters
In the early days of AI, the approach to machine intelligence relied on the idea of knowledge systems and manipulation of symbolic representations (also called GOFAI: Good Old Fashioned AI). In these systems, experts from their respective fields would incorporate knowledge into rule bases. With these rules, automated reasoning systems would logically derive the best outcomes. Because these systems were made by (human) experts, they worked pretty well, but lacked generality or failed to scale up when presented with larger problem sizes. Even the most succesful expert systems eventually collapsed under the pressure of underfunding in what is known as an AI winter. This trend of ups and downs can be seen throughout AIhistory, and many still believe it to be the case.

## Rise of Neural Networks
What followed, luckily for AI research, was the rise of approximate methods with the birth of Neural Networks. The idea, which can be traced back to the early 60's, gained traction with the derivation of continuous backpropagation and increased computational power. Over the last decade, as cheap, powerful GPU-based computing has become more available, this algorithm has proved to be _the_ general machine learning paradigm.

Together with the popularity of neural networks, the universal approximation theorem was sprouted. This theorem states that a neural network with a single hidden layer is able to approximate any continuous function, and thus should be able to perform any interesting form pattern recognition. Although, soon after, it was found that the amount of neurons in that layer may have to be infeasibly high, and moreover may fail to generalize properly.

With the birth of deep learning, researchers moved from wide to deep models. Instead of having a large amount of neurons in one or two layers, models are now considered to be more powerful when equipped with a multitude of fully connected, convolutional or pooling layers. All of these try to capture a certain structure in the task an AI developer employs the network for, and each of these layers is to be chosen carefully when designing a network.

## High Performance Machine Learning
Nowadays, researchers don't shy away from training huge models. These models, some with over a thousand layers, are notoriously difficult to train within reasonable time. For this, simple GPU acceleration is no longer sufficient. Even though matrix multiplication, the most important operation in the gradient descent update step, can be massively parallelized, single GPU performance falls short of the ever increasing need of computing power from AI researchers.

For this reason, frameworks such as [Horovod](https://github.com/horovod/horovod) have been introduced. The idea is to provide an easily scalable interface on top of existing machine learning frameworks (e.g. PyTorch or TensorFlow) such that minimal code adjustments are needed to perform distributed training. In this distributed setting, multiple GPUs can be employed in parallel to perform indepdendent gradient updates, which finally are averaged to form a single optimizer step.

--

Michiel van der Meer